Robots.txt — это текстовый файл, который содержит параметры индексирования сайта для роботов поисковых систем. В robots.txt можно ограничить индексирование роботами страниц сайта, что может снизить нагрузку на сайт и ускорить его работу.

располагается в корневой директории!

Директива	Что делает
User-agent *	Указывает на робота, для которого действуют перечисленные в robots.txt правила.
Disallow	Запрещает обход разделов или отдельных страниц сайта.
Sitemap		Указывает путь к файлу Sitemap, который размещен на сайте.
Clean-param	Указывает роботу, что URL страницы содержит параметры (например, UTM-метки), которые не нужно учитывать при индексировании.
Allow		Разрешает индексирование разделов или отдельных страниц сайта.
Crawl-delay	Задает роботу минимальный период времени (в секундах) между окончанием загрузки одной страницы и началом загрузки следующей.

пример:
	User-agent: * #указывает, для каких роботов установлены директивы
	Disallow: /bin/ # запрещает ссылки из "Корзины с товарами".
	Disallow: /search/ # запрещает ссылки страниц встроенного на сайте поиска
	Disallow: /admin/ # запрещает ссылки из панели администратора
	Sitemap: http://example.com/sitemap # указывает роботу на файл Sitemap для сайта
	Clean-param: ref /some_dir/get_book.pl
