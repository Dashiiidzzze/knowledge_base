устанавилваем библиотеки:
в виртуальное окружение:
`pip install requests` - для отправления запросов на сервер
`pip install bs4` - для парсинга данных
если выходит ошибка при установке нужно заменить команды на:
`sudo apt install python3-requests` и `sudo apt install python3-bs4`

Парсинг — это процесс сбора, обработки и анализа данных. В качестве их источника может выступать веб-сайт.

### методы запросов:
- GET - получение данных
- POST - отправка и получение данных
- (они описаны в [[Протокол HTTP]])

### получение ответа от сайта:
`respponse = requests.get(link)` - получение всех данных (и заголовков и ответа и всего всего)
`respponse = requests.get(link).text` - получение данных в виде текста
`respponse = requests.get(link).content` - получение данных в виде байтов (например для сохранения фото и других документов)

### где посмотреть ответ в браузере?
во вкладке сеть в разделе "ответ" будет все, что отправится

### просмотр ответа в коде:
`print(response.status_code)` - вывод кода ответа [[status code (код ошибки)]]
`print(response.text)` - вывод содержимого страницы в виде текста

### пример простого кода:
```Python
# импортируем пакет запросов
import requests

# ссылка на сайт
link = "https://icanhazip.com/"

# делаем запрос всей информации
responce = requests.get(link)

# вывод данных запроса
print(responce.status_code) # вывод кода ошибки
print(responce.text) # вывод текста

link2 = "https://browser-info.ru/"

# запрос данных в виде текста
responce2 = requests.get(link2).text

print(responce2)

# перевод текста в html в файл
with open("1.html", "w", encoding="utf-8") as file:
	file.write(responce2)
```

для применения BeautifulSoup установим
`sudo apt install python3-lxml` - парсер страницы в формат lxml

### подделка user-agent:
`pip install fake-useragent` - установка пакета для подделки
```Python
import requests
import fake_useragent
from bs4 import BeautifulSoup

# получение каждый раз разного юзерагента
user = fake_useragent.UserAgent().random
header = {'user-agent': user}
link = "https://browser-info.ru/"
response = requests.get(link, headers = header).text
```

### Авторизация:
Для авторизации нужно узнать ссылку на страницу входа и структуру post запроса.
```Python
import requests
import fake_useragent
from bs4 import BeautifulSoup

link = "http://forum.ru-board.com/misc.cgi?action=login"
user = fake_useragent.UserAgent().random
header = {
	'user-agent': user
}
data = {
	'action': 'dologin',
	'inmembername': 'dashidze',
	'inpassword': 'G4pc94'
}
responce = requests.post(link, data=data, headers=header)
print(responce.status_code)
print(responce.text)
```

### Сессии:
применяются для продолжения действий под логином.
Записываются кукисы и отправляются на сайт повторно вместе с запросом
Чтобы каждый раз не авторизовываться в аккаунте можно сохранить кукисы в словаре/документе и просто подгружать их.
```Python
import requests
import fake_useragent
from bs4 import BeautifulSoup

session = requests.Session() # для сохранения сессии
link = "http://forum.ru-board.com/misc.cgi?action=login"
user = fake_useragent.UserAgent().random
# заголовки:
header = {
	'user-agent': user
}
# тело запроса
data = {
	'action': 'dologin',
	'inmembername': 'dashidze',
	'inpassword': 'G4pc94'
}
# отправка запроса с сессией
responce = session.post(link, data=data, headers=header)
print(responce.status_code)
# print(responce.text)

# отправка запроса с сохраненной сессией
profile_link = "http://forum.ru-board.com/profile.cgi"
profile_responce = session.get(profile_link, data=data, headers=header)
print(profile_responce.status_code)
# print(profile_responce.text)

# словарь для хранения куков (их можно и в файл записать)
cookies_dict = [
	{"domain": key.domain, "name": key.name, "path": key.path, "value": key.value}
	for key in session.cookies
]
# достаем кукисы из словаря для новой сессии
session2 = requests.Session()
for cookies in cookies_dict:
	session2.cookies.set(**cookies)

# отправляем запрос с куками
responce2 = session2.get(profile_link, headers=header)
print(responce2.status_code)

# парсим имяп ользователя
soup = BeautifulSoup(responce2.text, "lxml")
block = soup.find('form')
head = block.find_all('tr')[0]
name = head.find('b').text
print(name)
```

### Скачивание файлов:
для скачивания фото нужно скачивать страничку в формате .content (байты)
```Python
import requests
import fake_useragent
from bs4 import BeautifulSoup

image_number = 0
link = "https://zastavok.net"

for storage_number in range(1, 6 + 1):
	header = {
		'User-Agent': fake_useragent.UserAgent().random,
		'Referer': link,
		'Accept-Language': 'en-US,en;q=0.9',
		'Connection': 'keep-alive'
	}
	responce = requests.get(f'{link}/search/Dodge+challenger/{storage_number}', headers=header, verify=False).text
	soup = BeautifulSoup(responce, 'lxml')
	block = soup.find('div', class_ = 'main')
	all_images = block.find_all('div', class_ = 'short_full')
	
	for image in all_images:
		link_image = image.find('a').get('href')
		image_resp = requests.get(f'{link}{link_image}', headers=header, verify=False).text
		download_soup = BeautifulSoup(image_resp, 'lxml')
		download_block = download_soup.find('div', class_ = 'image_data').find('div', class_ = 'block_down')
		result_link = download_block.find('a').get('href')
		
		# скачивание странички в виде байтов (фоточки)
		image_bytes = requests.get(f'{link}{result_link}', headers=header, verify=False).content
		
		# записываем файлфотчки
		with open(f'../../../Pictures/Dodge_challenger/{image_number}.jpg', 'wb') as file:
			file.write(image_bytes)
		
		image_number += 1
```

